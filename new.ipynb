{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9b94abd8-1efc-4e16-9f34-2dc3af1f2afb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3' max='3' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3/3 00:05, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "('./results\\\\tokenizer_config.json',\n",
       " './results\\\\special_tokens_map.json',\n",
       " './results\\\\vocab.txt',\n",
       " './results\\\\added_tokens.json')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BertForSequenceClassification, Trainer, TrainingArguments, BertTokenizer\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# Define a custom dataset class\n",
    "class PrivacyPolicyDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            max_length=self.max_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# Example data\n",
    "texts = [\"Your privacy is protected.\", \"We do not collect and share your data with third parties.\"]\n",
    "labels = [1, 0]  # 1 for good, 0 for bad\n",
    "\n",
    "# Initialize tokenizer and dataset\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "dataset = PrivacyPolicyDataset(texts, labels, tokenizer, max_length=128)\n",
    "\n",
    "# Data loader\n",
    "dataloader = DataLoader(dataset, batch_size=2)\n",
    "\n",
    "# Initialize model\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
    "\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=2,\n",
    "    logging_dir='./logs'\n",
    ")\n",
    "\n",
    "# Initialize trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset\n",
    ")\n",
    "\n",
    "# Train model\n",
    "trainer.train()\n",
    "\n",
    "# Save the model and tokenizer\n",
    "model.save_pretrained('./results')\n",
    "tokenizer.save_pretrained('./results')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1974bb71-a6be-484a-970f-4d46930747ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'LABEL_0', 'score': 0.5905711054801941}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Load fine-tuned model\n",
    "model = BertForSequenceClassification.from_pretrained('./results')\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Create a pipeline for text classification\n",
    "classifier = pipeline('text-classification', model=model, tokenizer=tokenizer)\n",
    "\n",
    "# New privacy policy\n",
    "new_policy = \"Your privacy is not protected, We collect and share your data with third parties.\"\n",
    "\n",
    "# Get prediction\n",
    "result = classifier(new_policy)\n",
    "print(result)  # This will print the predicted label and score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9b571bc2-ea98-4a3f-b394-eb1aec05d742",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\sspoo\\anaconda3\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\sspoo\\anaconda3\\Lib\\site-packages\\tf_keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some weights or buffers of the TF 2.0 model TFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "WARNING:tensorflow:AutoGraph could not transform <function infer_framework at 0x000002309AFCD120> and will run it as-is.\n",
      "Cause: for/else statement not yet supported\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function infer_framework at 0x000002309AFCD120> and will run it as-is.\n",
      "Cause: for/else statement not yet supported\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING:tensorflow:From C:\\Users\\sspoo\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\autograph\\converters\\directives.py:126: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\sspoo\\anaconda3\\Lib\\site-packages\\tensorflow\\python\\autograph\\converters\\directives.py:126: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From C:\\Users\\sspoo\\anaconda3\\Lib\\site-packages\\tf_keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "1/1 [==============================] - 80s 80s/step - loss: 0.7061 - accuracy: 0.5000\n",
      "Epoch 2/3\n",
      "1/1 [==============================] - 1s 690ms/step - loss: 0.6037 - accuracy: 0.5000\n",
      "Epoch 3/3\n",
      "1/1 [==============================] - 1s 691ms/step - loss: 0.4969 - accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('./results\\\\tokenizer_config.json',\n",
       " './results\\\\special_tokens_map.json',\n",
       " './results\\\\vocab.txt',\n",
       " './results\\\\added_tokens.json')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BertTokenizer, TFBertForSequenceClassification\n",
    "from transformers import glue_convert_examples_to_features, glue_processors\n",
    "import tensorflow as tf\n",
    "\n",
    "# Example data\n",
    "texts = [\"Your privacy is important to us.\", \"We collect and share your data.\"]\n",
    "labels = [1, 0]  # 1 for good, 0 for bad\n",
    "\n",
    "# Tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Tokenize and encode the data\n",
    "encodings = tokenizer(texts, truncation=True, padding=True, max_length=128)\n",
    "\n",
    "# Convert to TensorFlow Dataset\n",
    "def convert_to_tf_dataset(encodings, labels):\n",
    "    return tf.data.Dataset.from_tensor_slices((\n",
    "        dict(encodings),\n",
    "        labels\n",
    "    ))\n",
    "\n",
    "train_dataset = convert_to_tf_dataset(encodings, labels).shuffle(len(texts)).batch(2)\n",
    "\n",
    "# Load pre-trained BERT model\n",
    "model = TFBertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=2)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=5e-5),\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(train_dataset, epochs=3)\n",
    "\n",
    "# Save the model and tokenizer\n",
    "model.save_pretrained('./results')\n",
    "tokenizer.save_pretrained('./results')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c0b2e4d1-88bb-4fb0-b878-365e9d0bb4e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "All PyTorch model weights were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "All the weights of TFBertForSequenceClassification were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForSequenceClassification for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'label': 'LABEL_0', 'score': 0.6175602674484253}]\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, TFBertForSequenceClassification, pipeline\n",
    "\n",
    "# Load the fine-tuned model and tokenizer\n",
    "model = TFBertForSequenceClassification.from_pretrained('./results')\n",
    "tokenizer = BertTokenizer.from_pretrained('./results')\n",
    "\n",
    "# Create a pipeline for text classification\n",
    "classifier = pipeline('text-classification', model=model, tokenizer=tokenizer)\n",
    "\n",
    "# New privacy policy\n",
    "new_policy = \"We ensure your data is protected and not shared with third parties.\"\n",
    "\n",
    "# Get prediction\n",
    "result = classifier(new_policy)\n",
    "print(result)  # This will print the predicted label and score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4cc97289-34d8-4a2d-9d63-60d7957933db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import numpy as np\n",
    "\n",
    "# Example data (replace this with your actual data)\n",
    "data = [\n",
    "    ['hello', 'world'],\n",
    "    ['example', 'sentence']\n",
    "]\n",
    "\n",
    "# Train Word2Vec model\n",
    "model = gensim.models.Word2Vec(data, min_count=1, vector_size=2, window=5)\n",
    "\n",
    "# Get the word vectors and corresponding vocabulary\n",
    "word_vectors = model.wv\n",
    "vocabulary = word_vectors.index_to_key\n",
    "\n",
    "# Save word vectors to a text file\n",
    "output_file_path = \"word_vectors.txt\"\n",
    "\n",
    "with open(output_file_path, 'w', encoding='utf-8') as file:\n",
    "    # Write header: number of words and vector dimensionality\n",
    "    file.write(f\"{len(vocabulary)} {model.vector_size}\\n\")\n",
    "    \n",
    "    # Write each word and its corresponding vector\n",
    "    for word in vocabulary:\n",
    "        vector = ' '.join(map(str, word_vectors[word]))\n",
    "        file.write(f\"{word} {vector}\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
