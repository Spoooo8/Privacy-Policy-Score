{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8051d3c6-67f2-4e95-a1bf-78cbde509dde",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "All PyTorch model weights were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "Some weights or buffers of the TF 2.0 model TFBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pair 1:\n",
      "Sentence 1: We process your personal data strictly in accordance with the applicable laws and regulations. Your data will only be used for the following lawful purposes: With Your Consent: We will process your personal data when you have provided your explicit consent for specific purposes. Your consent will be obtained in a clear and transparent manner, and you will have the option to withdraw it at any time.\n",
      "Sentence 2: We will ensure that you are fully informed about the specific personal data we intend to collect and the purpose for which it will be used. We will inform you of your rights, including how you can exercise these rights under the applicable sections of the law. This includes your right to access, correct, or delete your data, as well as any other rights you may have under the law.\n",
      "Predicted Similarity Score: 0.371888667345047\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from transformers import BertTokenizer, TFBertForSequenceClassification\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Define the input texts for the task\n",
    "texts = [\n",
    "    (\"We process your personal data strictly in accordance with the applicable laws and regulations. Your data will only be used for the following lawful purposes: With Your Consent: We will process your personal data when you have provided your explicit consent for specific purposes. Your consent will be obtained in a clear and transparent manner, and you will have the option to withdraw it at any time.\", \n",
    "     \"We will ensure that you are fully informed about the specific personal data we intend to collect and the purpose for which it will be used. We will inform you of your rights, including how you can exercise these rights under the applicable sections of the law. This includes your right to access, correct, or delete your data, as well as any other rights you may have under the law.\"),\n",
    "    \n",
    "]\n",
    "\n",
    "# Load and tokenize the dataset\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples['sentence1'], examples['sentence2'], truncation=True, padding='max_length', max_length=128)\n",
    "\n",
    "# Create a custom dataset from text pairs\n",
    "def create_custom_dataset(text_pairs):\n",
    "    inputs = tokenizer([pair[0] for pair in text_pairs], [pair[1] for pair in text_pairs], truncation=True, padding='max_length', max_length=128, return_tensors='tf')\n",
    "    labels = [1.0] * len(text_pairs)  # Dummy labels for example; adjust based on your task\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((dict(inputs), labels))\n",
    "    return dataset\n",
    "\n",
    "# Prepare the custom dataset\n",
    "custom_dataset = create_custom_dataset(texts)\n",
    "\n",
    "# Load the pre-trained BERT model\n",
    "model = TFBertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=1)  # Use num_labels=1 for similarity scoring\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=2e-5),\n",
    "              loss=tf.keras.losses.MeanSquaredError(),\n",
    "              metrics=['mean_squared_error'])\n",
    "\n",
    "# Function to make predictions\n",
    "def predict_similarity(text_pairs):\n",
    "    inputs = tokenizer([pair[0] for pair in text_pairs], [pair[1] for pair in text_pairs], return_tensors='tf', padding=True, truncation=True, max_length=128)\n",
    "    outputs = model(inputs)\n",
    "    predictions = outputs.logits.numpy().flatten()\n",
    "    return predictions\n",
    "\n",
    "# Example usage\n",
    "predicted_scores = predict_similarity(texts)\n",
    "for i, (text1, text2) in enumerate(texts):\n",
    "    print(f'Pair {i+1}:')\n",
    "    print(f'Sentence 1: {text1}')\n",
    "    print(f'Sentence 2: {text2}')\n",
    "    print(f'Predicted Similarity Score: {predicted_scores[i]}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
